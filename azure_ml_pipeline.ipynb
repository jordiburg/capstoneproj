{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4375d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96354a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"environmentName\": \"AzureCloud\",\n",
      "  \"homeTenantId\": \"cf36141c-ddd7-45a7-b073-111f66d0b30c\",\n",
      "  \"id\": \"1d374132-87d3-49d8-a13a-910b42de3dde\",\n",
      "  \"isDefault\": true,\n",
      "  \"managedByTenants\": [],\n",
      "  \"name\": \"Visual Studio Professional Subscription\",\n",
      "  \"state\": \"Enabled\",\n",
      "  \"tenantId\": \"cf36141c-ddd7-45a7-b073-111f66d0b30c\",\n",
      "  \"user\": {\n",
      "    \"name\": \"h.g.gunasekaran@avanade.com\",\n",
      "    \"type\": \"user\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!az account show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773072d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import mlflow\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml.entities import AmlCompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b1633c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authenticate\n",
    "credential = DefaultAzureCredential()\n",
    "# # Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=\"1d374132-87d3-49d8-a13a-910b42de3dde\",\n",
    "    resource_group_name=\"capstoneamazonrg\",\n",
    "    workspace_name=\"capstoneamazonml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa7c49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0fbcc38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/dataprep.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/dataprep.yml\n",
    "name: dataprep\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge\n",
    "dependencies:\n",
    "- python=3.11.5\n",
    "- pip=21.3.1\n",
    "- pandas~=1.3.0\n",
    "- scipy~=1.10.0\n",
    "- numpy~=1.22.0\n",
    "- pip:\n",
    "  - wheel~=0.38.1\n",
    "  - azureml-core==1.55.0.post1\n",
    "  - azureml-defaults==1.55.0\n",
    "  - azureml-mlflow==1.55.0\n",
    "  - azureml-telemetry==1.55.0\n",
    "  - azureml-automl-common-tools==1.55.0\n",
    "  - scikit-learn~=1.0.0\n",
    "  - joblib~=1.2.0\n",
    "  # vulnerabilities\n",
    "  - cryptography>=42.0.4\n",
    "  - certifi >= 2023.07.22\n",
    "  - requests >= 2.31.0\n",
    "  - nltk==3.8.1\n",
    "  - mlfow==2.10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae951755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./dependencies/dataprep.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/dataprep.yml\n",
    "channels:\n",
    "  - anaconda\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip=21.3.1\n",
    "  - pandas~=1.3.0\n",
    "  - scipy~=1.10.0\n",
    "  - numpy~=1.22.0\n",
    "  - pip:\n",
    "      - wheel~=0.38.1\n",
    "      - azureml-core==1.55.0.post1\n",
    "      - azureml-defaults==1.55.0\n",
    "      - azureml-mlflow==1.55.0\n",
    "      - azureml-telemetry==1.55.0\n",
    "      - azureml-automl-common-tools==1.55.0\n",
    "      - scikit-learn~=1.0.0\n",
    "      - joblib~=1.2.0\n",
    "      - nltk==3.8.1\n",
    "name: dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6359a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name data_prep_env is registered to workspace, the environment version is 1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "data_prep_env_name = \"data_prep_env\"\n",
    "\n",
    "data_prep_job_env = Environment(\n",
    "    name=data_prep_env_name,\n",
    "    description=\"For data preparation\",\n",
    "    tags={\"data_prep\": \"0.1\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"dataprep.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "data_prep_job_env = ml_client.environments.create_or_update(data_prep_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {data_prep_job_env.name} is registered to workspace, the environment version is {data_prep_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7153ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n",
      "AMLCompute with name di-cluster will be created, with compute size STANDARD_DS11_V2\n"
     ]
    }
   ],
   "source": [
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"di-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure Machine Learning compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure Machine Learning Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS11_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=1,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=120,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    print(\n",
    "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "    )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26aef1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Amazon Fine Food Review\n",
    "csv_path = 'Data/Raw/amazon_fine_food_reviews.csv'\n",
    "reviews = pd.read_csv(csv_path, header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a528c8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ProductId          UserId                      ProfileName  \\\n",
       "Id                                                                \n",
       "1   B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "2   B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "3   B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "4   B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "5   B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "6   B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "7   B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "8   B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "9   B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "    HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "Id                                                                    \n",
       "1                      1                       1      5  1303862400   \n",
       "2                      0                       0      1  1346976000   \n",
       "3                      1                       1      4  1219017600   \n",
       "4                      3                       3      2  1307923200   \n",
       "5                      0                       0      5  1350777600   \n",
       "6                      0                       0      4  1342051200   \n",
       "7                      0                       0      5  1340150400   \n",
       "8                      0                       0      5  1336003200   \n",
       "9                      1                       1      5  1322006400   \n",
       "10                     0                       0      5  1351209600   \n",
       "\n",
       "                                          Summary  \\\n",
       "Id                                                  \n",
       "1                           Good Quality Dog Food   \n",
       "2                               Not as Advertised   \n",
       "3                           \"Delight\" says it all   \n",
       "4                                  Cough Medicine   \n",
       "5                                     Great taffy   \n",
       "6                                      Nice Taffy   \n",
       "7   Great!  Just as good as the expensive brands!   \n",
       "8                          Wonderful, tasty taffy   \n",
       "9                                      Yay Barley   \n",
       "10                               Healthy Dog Food   \n",
       "\n",
       "                                                 Text  \n",
       "Id                                                     \n",
       "1   I have bought several of the Vitality canned d...  \n",
       "2   Product arrived labeled as Jumbo Salted Peanut...  \n",
       "3   This is a confection that has been around a fe...  \n",
       "4   If you are looking for the secret ingredient i...  \n",
       "5   Great taffy at a great price.  There was a wid...  \n",
       "6   I got a wild hair for taffy and ordered this f...  \n",
       "7   This saltwater taffy had great flavors and was...  \n",
       "8   This taffy is so good.  It is very soft and ch...  \n",
       "9   Right now I'm mostly just sprouting this so my...  \n",
       "10  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_small = reviews[1:101]\n",
    "csv_path_small = 'Data/Raw/amazon_fine_food_reviews_small.csv'\n",
    "reviews_small.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "02436a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\h.g.gunasekaran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Create deduplicated frame\n",
    "# First by sorting the values by their ProductId\n",
    "# Second by dropping duplicates using all columns (which in proven cases are mostly similar) but the product id (which we have seen it differs)\n",
    "# reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'])\n",
    "# reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'ProfileName', 'Score', 'Time', 'Summary', 'Text'])\n",
    "# Either keeping or removing Time is valid depending on the approach\n",
    "reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'Text'])\n",
    "\n",
    "# Convert time\n",
    "reviews_dd['DateTime']=reviews_dd['Time'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "\n",
    "# Remove null values\n",
    "reviews_dd.dropna(inplace=True)\n",
    "\n",
    "reviews_dd['Year']=reviews_dd['Time'].apply(lambda x: datetime.datetime.fromtimestamp(x).year)\n",
    "\n",
    "# Create empty column for binarized score\n",
    "reviews_dd['binary_score'] = 0\n",
    "\n",
    "# Replace only the positive reviews by 1, leave the others with 0 (being negative)\n",
    "reviews_dd.loc[reviews_dd['Score'] > 3,'binary_score'] = 1\n",
    "\n",
    "# Lemmatization and Stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer('english') # SnowballStemmer() and other options\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Removing the word 'not' from stopwords\n",
    "default_stopwords = set(stopwords.words('english'))\n",
    "#excluding some useful words from stop words list as we doing sentiment analysis\n",
    "excluding = set(['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "custom_stopwords = default_stopwords - excluding\n",
    "\n",
    "reviews_dd['CleanedText'] = reviews_dd['Text'].apply(lambda x:preprocess(x))\n",
    "\n",
    "# Removed SWR\n",
    "reviews_dd['CleanedSwrText'] = reviews_dd['Text'].apply(lambda x:prepare_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25fd4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_dir = \"./AzureML/DataPrep\"\n",
    "os.makedirs(dataprep_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3e8a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./AzureML/DataPrep/dataprep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dataprep_dir}/dataprep.py\n",
    "import argparse\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "def preprocess(sentence):\n",
    "     \n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    # sentence = sentence.replace('{html}',\"\") \n",
    "    # For Regex Pattern Object\n",
    "    cleanr = re.compile(r'<\\s*[^>]*\\s*>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    rem_hyp = re.sub(r'(\\w+)-(\\w+)', r'\\1 \\2', cleantext)\n",
    "    rem_punc = re.sub(r'[^\\w\\s]', '', rem_hyp)\n",
    "    # re_clean = re.sub(r'[^a-z0-9A-Z_]',' ', cleantext)\n",
    "    rem_http = re.sub(r'http\\S+', '', rem_punc)\n",
    "    rem_url = re.sub(r\"www.\\S+\", \" \", rem_http)\n",
    "    rem_pat = re.sub(\"\\s*\\b(?=\\w*(\\w)\\1{2,})\\w*\\b\",' ', rem_url)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_pat)\n",
    "\n",
    "    return rem_num\n",
    "\n",
    "def tokenize_data(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)  \n",
    "    return tokens\n",
    "\n",
    "def remove_stop_words(cleant_text):\n",
    "    \n",
    "    # Lemmatization and Stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer('english') # SnowballStemmer() and other options\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    #Removing the word 'not' from stopwords\n",
    "    default_stopwords = set(stopwords.words('english'))\n",
    "    #excluding some useful words from stop words list as we doing sentiment analysis\n",
    "    excluding = set(['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "                 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "                 \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "\n",
    "    custom_stopwords = default_stopwords - excluding\n",
    "    \n",
    "    tokens = tokenize_data(cleant_text)\n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in custom_stopwords]\n",
    "    # stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    # lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def prepare_text(text):\n",
    "    \n",
    "    cleant_text = preprocess(text)\n",
    "    remove_sw = remove_stop_words(cleant_text)\n",
    "\n",
    "    return remove_sw\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "   \n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    # enable autologging\n",
    "    mlflow.sklearn.autolog()\n",
    "\n",
    "    ###################\n",
    "    #<prepare the data>\n",
    "    ###################\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "    \n",
    "    reviews = pd.read_csv(args.data, header=0, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", reviews.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", reviews.shape[1] - 1)\n",
    "    \n",
    "    print(reviews.columns)\n",
    "    \n",
    "    # Create deduplicated frame\n",
    "    # First by sorting the values by their ProductId\n",
    "    # Second by dropping duplicates using all columns (which in proven cases are mostly similar) but the product id (which we have seen it differs)\n",
    "    # reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'])\n",
    "    # reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'ProfileName', 'Score', 'Time', 'Summary', 'Text'])\n",
    "    # Either keeping or removing Time is valid depending on the approach\n",
    "    reviews_dd = reviews.sort_values(by='ProductId').drop_duplicates(subset=['UserId', 'Text'])\n",
    "\n",
    "    # Convert time\n",
    "    reviews_dd['DateTime']=reviews_dd['Time'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "\n",
    "    # Remove null values\n",
    "    reviews_dd.dropna(inplace=True)\n",
    "\n",
    "    reviews_dd['Year']=reviews_dd['Time'].apply(lambda x: datetime.datetime.fromtimestamp(x).year)\n",
    "\n",
    "    # Create empty column for binarized score\n",
    "    reviews_dd['binary_score'] = 0\n",
    "    \n",
    "    # Replace only the positive reviews by 1, leave the others with 0 (being negative)\n",
    "    reviews_dd.loc[reviews_dd['Score'] > 3,'binary_score'] = 1\n",
    "\n",
    "    reviews_dd['CleanedText'] = reviews_dd['Text'].apply(lambda x:preprocess(x))\n",
    "\n",
    "    # Removed SWR\n",
    "    reviews_dd['CleanedSwrText'] = reviews_dd['Text'].apply(lambda x:prepare_text(x))\n",
    "    \n",
    "    print(reviews_dd.head(10))\n",
    "    \n",
    "    reviews_dd_train, reviews_dd_test = train_test_split(\n",
    "        reviews_dd,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "    reviews_dd_train.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
    "\n",
    "    reviews_dd_test.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
    "\n",
    "    ####################\n",
    "    #</prepare the data>\n",
    "    ####################\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "16eddb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=\"./AzureML/DataPrep/\",  # location of source code\n",
    "    command=\"\"\"python dataprep.py \\\n",
    "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{data_prep_job_env.name}:7\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600855de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "reviews_model = reviews_dd.drop(['ProductId', 'UserId', 'ProfileName',\n",
    "       'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time',\n",
    "       'Summary', 'Text', 'DateTime', 'Year',\n",
    "       'CleanedText'], axis=1)\n",
    "reviews_model.dropna(inplace=True)\n",
    "\n",
    "reviews_model.head()\n",
    "\n",
    "# Define training datat and labels\n",
    "X = reviews_model['CleanedSwrText']\n",
    "y = reviews_model['binary_score']\n",
    "\n",
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30 ,random_state=99)\n",
    "\n",
    "# Use CountVectorizer to convert reviews into matrices\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "# equivalent to:\n",
    "# vect.fit(X_train) # words that are on the training set\n",
    "# X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "# Perform the same in test\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# Use Naive Bayes to predict binary score\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# Calculate accuracy.\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))\n",
    "\n",
    "print(classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0e03223",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./AzureML/Train\"\n",
    "os.makedirs(train_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d5a6c9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./AzureML/Train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB         # Naive Bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"binary_score\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.pop(\"CleanedSwrText\")\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"binary_score\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.pop(\"CleanedSwrText\")\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "    \n",
    "    # Use CountVectorizer to convert reviews into matrices\n",
    "    vect = CountVectorizer()\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "    # equivalent to:\n",
    "    # vect.fit(X_train) # words that are on the training set\n",
    "    # X_train_dtm = vect.transform(X_train)\n",
    "\n",
    "    # Perform the same in test\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "    # Use Naive Bayes to predict binary score\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred = nb.predict(X_test_dtm)\n",
    "    \n",
    "    # Calculate accuracy.\n",
    "    print((metrics.accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=nb,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=nb,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7085f443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./AzureML/Train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_dir}/train.yml\n",
    "# <component>\n",
    "name: review_classifier_model\n",
    "display_name: Review Classifier Model\n",
    "# version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder  \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}\n",
    "# </component>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6286e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Train (0.0 MBs): 100%|##########| 3886/3886 [00:00<00:00, 8237.70it/s]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component review_classifier_model with Version 2024-03-07-14-24-11-2352776 is registered\n"
     ]
    }
   ],
   "source": [
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "train_component = load_component(source=os.path.join(train_dir, \"train.yml\"))\n",
    "\n",
    "# Now we register the component to the workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ae92684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure Machine Learning pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target,\n",
    "    description=\"E2E data_perp-train pipeline\",\n",
    ")\n",
    "def reviews_rating_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = train_component(\n",
    "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07838f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: .\\config.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(credential=DefaultAzureCredential())\n",
    "data_asset = ml_client.data.get(\"AmazonFineFoodReivewsSmall\", version=\"1\")\n",
    "\n",
    "registered_model_name = \"reviews_rating_classifier\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = reviews_rating_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=data_asset.path),\n",
    "    pipeline_job_test_train_ratio=0.25,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2efb8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: quiet_garlic_498b6jswxb\n",
      "Web View: https://ml.azure.com/runs/quiet_garlic_498b6jswxb?wsid=/subscriptions/1d374132-87d3-49d8-a13a-910b42de3dde/resourcegroups/capstoneamazonrg/workspaces/capstoneamazonml\n",
      "\n",
      "Streaming logs/azureml/executionlogs.txt\n",
      "========================================\n",
      "\n",
      "[2024-03-07 14:45:44Z] Completing processing run id 601e9191-5ba8-441d-ae30-a699e1d3c00f.\n",
      "[2024-03-07 14:45:45Z] Submitting 1 runs, first five are: 278dfb87:c7d33b28-e67c-4522-aa2d-f458d5844469\n",
      "[2024-03-07 14:50:31Z] Completing processing run id c7d33b28-e67c-4522-aa2d-f458d5844469.\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: quiet_garlic_498b6jswxb\n",
      "Web View: https://ml.azure.com/runs/quiet_garlic_498b6jswxb?wsid=/subscriptions/1d374132-87d3-49d8-a13a-910b42de3dde/resourcegroups/capstoneamazonrg/workspaces/capstoneamazonml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b75f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Create a unique name for the endpoint\n",
    "online_endpoint_name = \"rating-endpointt-\" + str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d31dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineEndpoint\n",
    "\n",
    "# define an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is an online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"ratings_defaults\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fdac96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the online endpoint\n",
    "# expect the endpoint to take approximately 2 minutes.\n",
    "\n",
    "endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3c3cbf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint \"rating-endpointt-ccd406b9\" with provisioning state \"Succeeded\" is retrieved\n"
     ]
    }
   ],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "print(\n",
    "    f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7adbfa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   from azure.ai.ml.entities import (\n",
    "       AutoPauseSettings,\n",
    "       AutoScaleSettings,\n",
    "       DefaultScaleSettings,\n",
    "       IdentityConfiguration,\n",
    "       ManagedIdentityConfiguration,\n",
    "       SynapseSparkCompute,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c32955d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import ManagedOnlineDeployment\n",
    "\n",
    "registered_model_name = \"reviews_rating_classifier\"\n",
    "\n",
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max(\n",
    "    [int(m.version) for m in ml_client.models.list(name=registered_model_name)]\n",
    ")\n",
    "\n",
    "print(latest_model_version)\n",
    "\n",
    "# Choose the latest version of our registered model for deployment\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)\n",
    "\n",
    "# define an online deployment\n",
    "test_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    instance_type=\"Standard_D2as_v4\", \n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f839dbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint rating-endpointt-ccd406b9 exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Readonly attribute principal_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n",
      "Readonly attribute tenant_id will be ignored in class <class 'azure.ai.ml._restclient.v2022_05_01.models._models_py3.ManagedServiceIdentity'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://rating-endpointt-ccd406b9.germanywestcentral.inference.ml.azure.com/score', 'openapi_uri': 'https://rating-endpointt-ccd406b9.germanywestcentral.inference.ml.azure.com/swagger.json', 'name': 'rating-endpointt-ccd406b9', 'description': 'this is an online endpoint', 'tags': {'training_dataset': 'ratings_defaults'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/1d374132-87d3-49d8-a13a-910b42de3dde/resourcegroups/capstoneamazonrg/providers/microsoft.machinelearningservices/workspaces/capstoneamazonml/onlineendpoints/rating-endpointt-ccd406b9', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/1d374132-87d3-49d8-a13a-910b42de3dde/providers/Microsoft.MachineLearningServices/locations/germanywestcentral/mfeOperationsStatus/oe:a47cb470-a91b-4883-9a14-14ecf10908a8:9b0fd2ea-1127-4a61-a168-334e6a02c486?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/1d374132-87d3-49d8-a13a-910b42de3dde/resourceGroups/capstoneamazonrg/providers/Microsoft.MachineLearningServices/workspaces/capstoneamazonml/onlineEndpoints/rating-endpointt-ccd406b9', 'Resource__source_path': None, 'base_path': 'C:\\\\Users\\\\h.g.gunasekaran\\\\Documents\\\\Capstone2\\\\capstoneproj', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x000001D554AE6A90>, 'auth_mode': 'key', 'location': 'germanywestcentral', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x000001D5520B3890>, 'traffic': {'blue': 100}, 'mirror_traffic': {}, 'kind': 'Managed'})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the online deployment\n",
    "test_deployment = ml_client.online_deployments.begin_create_or_update(\n",
    "    test_deployment\n",
    ").result()\n",
    "\n",
    "\n",
    "# blue deployment takes 100% traffic\n",
    "# expect the deployment to take approximately 8 to 10 minutes.\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "25d51a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: rating-endpointt-ccd406b9\n",
      "Status: Succeeded\n",
      "Description: this is an online endpoint\n"
     ]
    }
   ],
   "source": [
    "# return an object that contains metadata for the endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "\n",
    "# print a selection of the endpoint's metadata\n",
    "print(\n",
    "    f\"Name: {endpoint.name}\\nStatus: {endpoint.provisioning_state}\\nDescription: {endpoint.description}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8022a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# Create a directory to store the sample request file.\n",
    "deploy_dir = \"./AzureML/Resources\"\n",
    "os.makedirs(deploy_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "test_dict = {\n",
    "  \"input_data\": {\n",
    "    \"columns\": [\"CleanedSwrText\"],\n",
    "    \"index\": [0],\n",
    "    \"data\": [[\"movie good\"]]\n",
    "    }\n",
    "}\n",
    "json_object = json.dumps(test_dict, indent=4)\n",
    "with open(f\"{deploy_dir}/sample-request.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2965def8",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(None) An unexpected error occurred in scoring script. Check the logs for more info.\nCode: None\nMessage: An unexpected error occurred in scoring script. Check the logs for more info.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test the blue deployment with the sample data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ml_client\u001b[38;5;241m.\u001b[39monline_endpoints\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m      3\u001b[0m     endpoint_name\u001b[38;5;241m=\u001b[39monline_endpoint_name,\n\u001b[0;32m      4\u001b[0m     deployment_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     request_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./AzureML/Resources/sample-request.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\_telemetry\\activity.py:275\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[1;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\operations\\_online_endpoint_operations.py:357\u001b[0m, in \u001b[0;36mOnlineEndpointOperations.invoke\u001b[1;34m(self, endpoint_name, request_file, deployment_name, input_data, params_override, local, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m     headers[EndpointInvokeFields\u001b[38;5;241m.\u001b[39mMODEL_DEPLOYMENT] \u001b[38;5;241m=\u001b[39m deployment_name\n\u001b[0;32m    356\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requests_pipeline\u001b[38;5;241m.\u001b[39mpost(endpoint\u001b[38;5;241m.\u001b[39mproperties\u001b[38;5;241m.\u001b[39mscoring_uri, json\u001b[38;5;241m=\u001b[39mdata, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m--> 357\u001b[0m validate_response(response)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\azure\\ai\\ml\\_utils\\_endpoint_utils.py:131\u001b[0m, in \u001b[0;36mvalidate_response\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    123\u001b[0m failure_msg \u001b[38;5;241m=\u001b[39m r_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m    124\u001b[0m error_map \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;241m401\u001b[39m: ClientAuthenticationError,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;241m404\u001b[39m: ResourceNotFoundError,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;241m424\u001b[39m: HttpResponseError,\n\u001b[0;32m    130\u001b[0m }\n\u001b[1;32m--> 131\u001b[0m map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, message\u001b[38;5;241m=\u001b[39mfailure_msg, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\azure\\core\\exceptions.py:164\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    163\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mHttpResponseError\u001b[0m: (None) An unexpected error occurred in scoring script. Check the logs for more info.\nCode: None\nMessage: An unexpected error occurred in scoring script. Check the logs for more info."
     ]
    }
   ],
   "source": [
    "# test the blue deployment with the sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"blue\",\n",
    "    request_file=\"./AzureML/Resources/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c6467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems we have to process the to be predicted data too and then feed it to the model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
